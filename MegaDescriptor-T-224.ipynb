{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "672d8d6f",
   "metadata": {},
   "source": [
    "# Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aebe29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from prepare_data import *\n",
    "\n",
    "prepare_functions = {\n",
    "    # 'NyalaData': prepare_nyala_data,\n",
    "    # 'ZindiTurtleRecall': prepare_zindi_turtle_recall,\n",
    "    # 'BelugaID': prepare_beluga_id,\n",
    "    # 'BirdIndividualID': prepare_bird_individual_id,\n",
    "    # 'SealID': prepare_seal_id,\n",
    "    # 'FriesianCattle2015': prepare_friesian_cattle_2015,\n",
    "    # 'ATRW': prepare_atrw,\n",
    "    # 'NDD20': prepare_ndd20,\n",
    "    # 'SMALST': prepare_smalst,\n",
    "    # 'SeaTurtleIDHeads': prepare_sea_turtle_id_heads,\n",
    "    # 'AAUZebraFish': prepare_zebra_fish,\n",
    "    # 'CZoo': prepare_czoo,\n",
    "    # 'CTai': prepare_ctai,\n",
    "    # 'Giraffes': prepare_giraffes,\n",
    "    # 'HyenaID2022': prepare_hyena_id_2022,\n",
    "    # 'MacaqueFaces': prepare_macaque_faces,\n",
    "    # 'OpenCows2020': prepare_open_cows_2020,\n",
    "    # 'StripeSpotter': prepare_stripe_spotter,\n",
    "    # 'AerialCattle2017': prepare_aerial_cattle_2017,\n",
    "    # 'GiraffeZebraID': prepare_giraffe_zebra_id,\n",
    "    # 'IPanda50': prepare_ipanda_50,\n",
    "    # 'WhaleSharkID': prepare_whaleshark_id,\n",
    "    # 'FriesianCattle2017': prepare_friesian_cattle_2017,\n",
    "    # 'Cows2021': prepare_cows2021,\n",
    "    # 'LeopardID2022': prepare_leopard_id_2022,\n",
    "    # 'NOAARightWhale': prepare_noaa_right_whale,\n",
    "    # 'HappyWhale': prepare_happy_whale,\n",
    "    # 'HumpbackWhaleID': prepare_humpback_whale_id,\n",
    "    # 'LionData': prepare_lion_data\n",
    "    'MPDD': prepare_mpdd,\n",
    "}\n",
    "\n",
    "datasets_folder = 'data'  # Path to downloaded datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d9e25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPDD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 1657/1657 [00:21<00:00, 75.62it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create folders with images resized to 256 and 518\n",
    "for name, prepare in prepare_functions.items():\n",
    "    print(name)\n",
    "    prepare(size=256, root= fr'{datasets_folder}\\{name}', new_root=fr'images\\size-256\\{name}')\n",
    "    # prepare(size=518, root=fr'{datasets_folder}\\{name}', new_root=fr'images\\size-518\\{name}')\n",
    "\n",
    "    # Metadata should be the same\n",
    "    # metadata_256 = pd.read_csv(fr'images\\size-256\\{name}\\annotations.csv', index_col=0)\n",
    "    # metadata_518 = pd.read_csv(f'images/size-518/{name}/annotations.csv', index_col=0)\n",
    "    # assert metadata_256.equals(metadata_518)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to add metadata files. Can also download them directly from the dataset page\n",
    "\n",
    "\n",
    "# # Create dataframe with training / test set splits\n",
    "# from wildlife_datasets import splits\n",
    "# for name in prepare_functions:\n",
    "#     metadata = pd.read_csv(f'images/size-256/{name}/annotations.csv', index_col=0)\n",
    "#     splitter = splits.ClosedSetSplit(0.8, identity_skip='unknown', seed=666)\n",
    "#     idx_train, idx_test = splitter.split(metadata)[0]\n",
    "\n",
    "#     metadata.loc[metadata.index[idx_train], 'split'] = 'train'\n",
    "#     metadata.loc[metadata.index[idx_test], 'split'] = 'test'\n",
    "\n",
    "#     os.makedirs(f'metadata/datasets/{name}/', exist_ok=True)\n",
    "#     metadata.to_csv(f'metadata/datasets/{name}/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25bf6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class ArcFaceLoss(nn.Module):\n",
    "#     def __init__(self, num_classes, embedding_size, margin=0.5, scale=64.0):\n",
    "#         super(ArcFaceLoss, self).__init__()\n",
    "#         self.num_classes = num_classes\n",
    "#         self.embedding_size = embedding_size\n",
    "#         self.margin = margin\n",
    "#         self.scale = scale\n",
    "#         self.W = nn.Parameter(torch.randn(embedding_size, num_classes))\n",
    "\n",
    "#     def forward(self, embeddings, labels):\n",
    "#         # Normalize input embeddings\n",
    "#         embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "#         # Normalize weights\n",
    "#         W = F.normalize(self.W, p=2, dim=0)\n",
    "#         # Compute cosine similarity between embeddings and weights\n",
    "#         cosine = torch.matmul(embeddings, W)\n",
    "#         # Add margin to cosine similarity for target classes\n",
    "#         cosine_target = cosine - self.margin * (torch.eq(labels.view(-1,1), torch.arange(self.num_classes).to(labels.device)).float() - cosine)\n",
    "#         # Scale cosine similarity\n",
    "#         logits = self.scale * cosine_target\n",
    "#         # Compute cross-entropy loss\n",
    "#         loss = F.cross_entropy(logits, labels)\n",
    "#         return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2612220",
   "metadata": {},
   "source": [
    "# MegaDescriptor-T-224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9b9045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchvision import transforms as T\n",
    "from timm import create_model\n",
    "\n",
    "from wildlife_tools.features import DeepFeatures\n",
    "from wildlife_tools.data import WildlifeDataset, SplitMetadata\n",
    "from wildlife_tools.similarity import CosineSimilarity\n",
    "from wildlife_tools.inference import KnnClassifier\n",
    "import torch\n",
    "\n",
    "devic = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = create_model(\"hf-hub:BVRA/MegaDescriptor-T-224\", pretrained=True)\n",
    "extractor = DeepFeatures(model, device=devic)\n",
    "\n",
    "root_images = r'images\\size-256'\n",
    "root_metadata = r'metadata\\datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9137aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets = [\n",
    "    # 'BirdIndividualID',\n",
    "    # 'SealID',\n",
    "    # 'FriesianCattle2015',\n",
    "    # 'ATRW',\n",
    "    # 'NDD20',\n",
    "    # 'SMALST',\n",
    "    # 'SeaTurtleIDHeads',\n",
    "    # 'AAUZebraFish',\n",
    "    # 'CZoo',\n",
    "    # 'CTai',\n",
    "    # 'Giraffes',\n",
    "    # 'HyenaID2022',\n",
    "    # 'MacaqueFaces',\n",
    "    # 'OpenCows2020',\n",
    "    # 'StripeSpotter',\n",
    "    # 'AerialCattle2017',\n",
    "    # 'GiraffeZebraID',\n",
    "    # 'IPanda50',\n",
    "    # 'WhaleSharkID',\n",
    "    # 'FriesianCattle2017',\n",
    "    # 'Cows2021',\n",
    "    # 'LeopardID2022',\n",
    "    # 'NOAARightWhale',\n",
    "    # 'HappyWhale',\n",
    "    # 'HumpbackWhaleID',\n",
    "    'LionData',\n",
    "    # 'MPDD'\n",
    "    # 'NyalaData',\n",
    "    # 'ZindiTurtleRecall',\n",
    "    # 'BelugaID',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8543ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for name in datasets:\n",
    "    metadata = pd.read_csv(fr'{root_metadata}\\{name}\\metadata.csv', index_col=0)\n",
    "\n",
    "    transform = T.Compose([\n",
    "        T.Resize(size=(224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "    database = WildlifeDataset(\n",
    "        metadata=metadata,\n",
    "        root=f'{root_images}/{name}/',\n",
    "        transform=transform,\n",
    "        split=SplitMetadata('split', 'train'),\n",
    "    )\n",
    "\n",
    "    query = WildlifeDataset(\n",
    "        metadata=metadata,\n",
    "        root=f'{root_images}/{name}/',\n",
    "        transform=transform,\n",
    "        split=SplitMetadata('split', 'test'),\n",
    "    )\n",
    "    \n",
    "    database_embeddings = extractor(database)\n",
    "    query_embeddings = extractor(query)\n",
    "    print('-----')\n",
    "    print(database_embeddings.shape, query_embeddings.shape)\n",
    "    print('-----')\n",
    "    # arcface_loss = ArcFaceLoss(num_classes=40, embedding_size=1024, margin=0.2, scale=20)\n",
    "    # loss = arcface_loss(database_embeddings, database.labels)\n",
    "    \n",
    "    \n",
    "    matcher = CosineSimilarity()\n",
    "    similarity = matcher(query=query_embeddings, database=database_embeddings)\n",
    "    preds = KnnClassifier(k=1, database_labels=database.labels_string)(similarity['cosine'])\n",
    "    \n",
    "    acc = sum(preds == query.labels_string) / len(preds)\n",
    "    print(name, acc)\n",
    "    results['Accuracy'] = acc\n",
    "    # results['loss'] = acc\n",
    "    \n",
    "# pd.Series(results).to_csv('results/MegaDescriptor-T-224_FriesianCattle2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b05637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.14046082949308755\n",
      "Sensitivity (Recall): 0.14838709677419354\n",
      "{'Accuracy': 0.14838709677419354, 'F1 Score': 0.14046082949308755, 'Sensitivity (Recall)': 0.14838709677419354}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "f1 = f1_score(query.labels_string, preds, average='weighted')\n",
    "print(\"F1 Score:\", f1)\n",
    "results['F1 Score'] = f1\n",
    "\n",
    "sensitivity = recall_score(query.labels_string, preds, average='weighted')\n",
    "print(\"Sensitivity (Recall):\", sensitivity)\n",
    "results['Sensitivity (Recall)'] = sensitivity\n",
    "\n",
    "print(results)\n",
    "pd.Series(results).to_csv('results/MegaDescriptor-T-224_MPDD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bbb558e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9904458598726115,\n",
       " 'F1 Score': 0.9904458598726115,\n",
       " 'Sensitivity (Recall)': 0.9904458598726115}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity and specificity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab7cb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# do deep copy\n",
    "results_copy = copy.deepcopy(results)\n",
    "preds_copy = copy.deepcopy(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a8272bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'FriesianCattle2015': 0.55, 'F1 Score': 0.5433333333333333},\n",
       " array(['2', '12', '12', '16', '25', '20', '38', '23', '18', '8', '9',\n",
       "        '21', '3', '1', '18', '6', '17', '38', '23', '31', '1', '9', '19',\n",
       "        '40', '24', '37', '22', '7', '5', '8', '36', '7', '1', '8', '4',\n",
       "        '26', '36', '39', '8', '5', '18', '2', '13', '9', '21', '26', '7',\n",
       "        '2', '25', '11', '7', '13', '15', '16', '1', '2', '6', '6', '9',\n",
       "        '3', '21', '40', '25', '15', '24', '17', '26', '34', '5', '5',\n",
       "        '11', '19', '30', '10', '18', '20', '35', '23', '4', '10'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_copy,preds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c598d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity (Recall): 0.9904458598726115\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a9e4fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Dan', 'Dan', 'Dan', ..., 'Verity', 'Verity', 'Verity'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a= np.unique(preds)\n",
    "# a = [int(i) for i in a]\n",
    "# a.sort()\n",
    "# print(a)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55c1b00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macaque F1 Score: 0.990449728379437\n",
      "Sensitivity (Recall): 0.9904458598726115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9904458598726115,\n",
       " 'F1 Score': 0.990449728379437,\n",
       " 'Sensitivity (Recall)': 0.9904458598726115}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "f1 = f1_score(query.labels_string, preds, average='weighted')\n",
    "print(\"Macaque F1 Score:\", f1)\n",
    "results['F1 Score'] = f1\n",
    "\n",
    "sensitivity = recall_score(query.labels_string, preds, average='weighted')\n",
    "print(\"Sensitivity (Recall):\", sensitivity)\n",
    "results['Sensitivity (Recall)'] = sensitivity\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db403ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LionData F1 Score: 0.14046082949308755\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(query.labels_string, preds, average='weighted')\n",
    "print(\"LionData F1 Score:\", f1)\n",
    "results['F1 Score'] = f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bfd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch import nn\n",
    "\n",
    "# class ArcFaceLoss(nn.Module):\n",
    "#     def _init_(self, s=30.0, m=0.5):\n",
    "#         super(ArcFaceLoss, self)._init_()\n",
    "#         self.s = s\n",
    "#         self.m = m\n",
    "\n",
    "#     def forward(self, logits, labels):\n",
    "#         theta = torch.acos(torch.clamp(logits, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "#         target_theta = theta + self.m\n",
    "#         target_logits = torch.cos(target_theta)\n",
    "\n",
    "#         one_hot_labels = F.one_hot(labels, num_classes=logits.size(1))\n",
    "#         output = one_hot_labels * target_logits + (1.0 - one_hot_labels) * logits\n",
    "#         output *= self.s\n",
    "#         loss = F.cross_entropy(output, labels)\n",
    "#         return loss\n",
    "\n",
    "# # calculating arcface loss for the above predictions\n",
    "# arcface_loss = ArcFaceLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def _init_(self, margin=0.2):\n",
    "        super(TripletLoss, self)._init_()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = torch.sum((anchor - positive) ** 2, dim=1)\n",
    "        distance_negative = torch.sum((anchor - negative) ** 2, dim=1)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        loss = torch.mean(losses)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a05bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import WildlifeDataset\n",
    "import torchvision.transforms as T\n",
    "import pandas as pd\n",
    "\n",
    "# Define ArcFace loss function\n",
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, scale=30, margin=0.5):\n",
    "        super(ArcFaceLoss, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        cos_theta = logits\n",
    "        theta = torch.acos(cos_theta)\n",
    "        m_theta = torch.cos(theta + self.margin)\n",
    "        logits = self.scale * m_theta\n",
    "        return self.cross_entropy(logits, labels)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = YourModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "arcface_loss = ArcFaceLoss(scale=30, margin=0.5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_data in dataloader:\n",
    "        inputs, labels = batch_data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = arcface_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Model evaluation\n",
    "with torch.no_grad():\n",
    "    for name in datasets:\n",
    "        # Load dataset and calculate predictions\n",
    "        acc = calculate_accuracy(...)\n",
    "        print(name, acc)\n",
    "        results[name] = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51dca036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
